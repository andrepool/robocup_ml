[net]
# most of the informatin this [net] block is only used for training
batch=64
# for training subdivsion shall be smaller then 32 and not more than batch
# see https://github.com/AlexeyAB/darknet/wiki/YOLOv4-model-zoo
subdivisions=8

width=416
height=416

channels=3
momentum=0.949
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.0013
burn_in=1000
# use at least 600000
max_batches = 600000
policy=steps
steps=480000,540000
scales=.1,.1

#cutmix=1
mosaic=1

#:104x104 54:52x52 85:26x26 104:13x13 for 416

# for explanation hyper parameters see
# https://wikidocs.net/181736

# layer 0: 416x416x3 -> 416x416x16
[convolutional]
batch_normalize=1
filters=16
size=3
stride=1
pad=1
activation=mish

# Downsample
# layer 1: 416x416x16 -> 208x208x32
[convolutional]
batch_normalize=1
filters=32
size=3
stride=2
pad=1
activation=mish

# layer 2: 208x208x32 -> 208x208x32
[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=mish

# layer 3: restart from output layer 1 (208x208x32)
[route]
layers = -2

# layer 4: 208x208x32 -> 208x208x32
[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=mish

# layer 5: 208x208x32 -> 208x208x16
[convolutional]
batch_normalize=1
filters=16
size=1
stride=1
pad=1
activation=mish

# layer 6: 208x208x16 -> 208x208x32
[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=mish

# layer 7: add output layer 4 (208x208x32) to output layer 6 (208x208x32) -> 208x208x32
[shortcut]
from=-3
activation=linear

# layer 8: 208x208x32 -> 208x208x32
[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=mish

# layer 9: concatenate output layer 2 (208x208x32) with layer 8 (208x208x32) -> 208x208x64
# concatenate (batch, w, h, c1) with (batch, w, h, c2) = (batch, w, h, c1+c2)
[route]
layers = -1,-7

# layer 10: 208x208x64 -> 208x208x32
[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=mish

# Downsample
# layer 11: 208x208x32 -> 104x104x64
[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
activation=mish

# layer 12: 104x104x64 -> 104x104x32
[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=mish

# layer 13: restart from output layer 11 (104x104x64)
[route]
layers = -2

# layer 14: 104x104x64 -> 104x104x32
[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=mish

# layer 15: 104x104x32 -> 104x104x32
[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=mish

# layer 16: 104x104x32 -> 104x104x32
[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=mish

# layer 17: add output layer 14 (104x104x32) to output layer 16 (104x104x32) -> 104x104x32
[shortcut]
from=-3
activation=linear

# layer 18: 104x104x32 -> 104x104x32
[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=mish

# layer 19: 104x104x32 -> 104x104x32
[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=mish

# layer 20: add ouptut layer 17 (104x104x32) to output layer 19 (104x104x32) -> 104x104x32
[shortcut]
from=-3
activation=linear

# layer 21: 104x104x32 -> 104x104x32
[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=mish

# layer 22: concatenate output layer 12 (104x104x32) with outptut layer 21 (104x104x32) -> 104x104x64
[route]
layers = -1,-10

# layer 23: 104x104x64 -> 104x104x64
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=mish

# Downsample
# layer 24: 104x104x64 -> 52x52x128
[convolutional]
batch_normalize=1
filters=128
size=3
stride=2
pad=1
activation=mish

# layer 25: 52x52x128 -> 52x52x64
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=mish

# layer 26: restart from output layer 24 (52x52x128)
[route]
layers = -2

# layer 27: 52x52x64 -> 52x52x64
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=mish

# layer 28: 52x52x64 -> 52x52x64
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=mish

# layer 29: 52x52x64 -> 52x52x64
[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=mish

# layer 30: add output layer 27 (52x52x64) to output layer 29 (52x52x64) -> 52x52x64
[shortcut]
from=-3
activation=linear

# layer 31: 52x52x64 -> 52x52x64
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=mish

# layer 32: 52x52x64 -> 52x52x64
[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=mish

# layer 33: add output layer 30 (52x52x64) to output layer 32 (52x52x64) -> 52x52x64
[shortcut]
from=-3
activation=linear

# layer 34: 52x52x64 -> 52x52x64
# [convolutional]
# batch_normalize=1
# filters=64
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 35: 52x52x64 -> 52x52x64
# [convolutional]
# batch_normalize=1
# filters=64
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 36: add output layer 33 (52x52x64) to output layer 35 (52x52x64) -> 52x52x64
# [shortcut]
# from=-3
# activation=linear
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 37: 52x52x64 -> 52x52x64
# [convolutional]
# batch_normalize=1
# filters=64
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 38: 52x52x64 -> 52x52x64
# [convolutional]
# batch_normalize=1
# filters=64
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 39: add output layer 36 (52x52x64) to output layer 38 (52x52x64) -> 52x52x64
# [shortcut]
# from=-3
# activation=linear
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 40: 52x52x64 -> 52x52x64
# [convolutional]
# batch_normalize=1
# filters=64
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 41: 52x52x64 -> 52x52x64
# [convolutional]
# batch_normalize=1
# filters=64
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 42: add output layer 39 (52x52x64) to output layer 41 (52x52x64) -> 52x52x64
# [shortcut]
# from=-3
# activation=linear
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 43: 52x52x64 -> 52x52x64
# [convolutional]
# batch_normalize=1
# filters=64
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 44: 52x52x64 -> 52x52x64
# [convolutional]
# batch_normalize=1
# filters=64
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 45: add output layer 42 (52x52x64) to output layer 44 (52x52x64) -> 52x52x64
# [shortcut]
# from=-3
# activation=linear
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 46: 52x52x64 -> 52x52x64
# [convolutional]
# tch_normalize=1
# filters=64
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 47: 52x52x64 -> 52x52x64
# [convolutional]
# batch_normalize=1
# filters=64
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 48: add output layer 45 (52x52x64) to output layer 47 (52x52x64) -> 52x52x64
# [shortcut]
# from=-3
# activation=linear
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 49: 52x52x64 -> 52x52x64
# [convolutional]
# batch_normalize=1
# filters=64
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 50: 52x52x64 -> 52x52x64
# [convolutional]
# batch_normalize=1
# filters=64
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 51: add output layer 48 (52x52x64) to output layer 50 (52x52x64) -> 52x52x64
# [shortcut]
# from=-3
# activation=linear
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 52: 52x52x64 -> 52x52x64
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=mish

# layer 53: concatenate output layer 25 (52x52x64) with output layer 52 (52x52x64) -> 52x52x128
[route]
layers = -1,-28

# layer 54: 52x52x128 -> 52x52x128
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=mish

# Downsample
# layer 55: 52x52x128 -> 26x26x256
[convolutional]
batch_normalize=1
filters=256
size=3
stride=2
pad=1
activation=mish

# layer 56: 26x26x256 -> 26x26x128
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=mish

# layer 57: restart from output layer 55 (26x26x256)
[route]
layers = -2

# layer 58: 26x26x128 -> 26x26x128
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=mish

# layer 59: 26x26x128 -> 26x26x128
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=mish

# layer 60: 26x26x128 -> 26x26x128
[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=mish

# layer 61: add output layer 58 (26x26x128) to output layer 60 (26x26x128) -> 26x26x128
[shortcut]
from=-3
activation=linear

# layer 62: 26x26x128 -> 26x26x128
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=mish

# layer 63: 26x26x128 -> 26x26x128
[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=mish

# layer 64: add output layer 61 (26x26x128) to output layer 63 (26x26x128) -> 26x26x128
[shortcut]
from=-3
activation=linear

# layer 65: 26x26x128 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 66: 26x26x128 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 67: add output layer 64 (26x26x128) to output layer 66 (26x26x128) -> 26x26x128
# [shortcut]
# from=-3
# activation=linear
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 68: 26x26x128 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 69: 26x26x128 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 70: add output layer 67 (26x26x128) to output layer 69 (26x26x128) -> 26x26x128
[shortcut]
from=-3
activation=linear

# layer 71: 26x26x128 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear


# layer 72: 26x26x128 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 73: add output layer 70 (26x26x128) to output layer 72 (26x26x128) -> 26x26x128
# [shortcut]
# from=-3
# activation=linear
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 74: 26x26x128 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 75: 26x26x128 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 76: add output layer 73 (26x26x128) to output layer 75 (26x26x128) -> 26x26x128
# [shortcut]
# from=-3
# activation=linear
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 77: 26x26x128 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 78: 26x26x128 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 79: add output layer 76 (26x26x128) to output layer 78 (26x26x128) -> 26x26x128
# [shortcut]
# from=-3
# activation=linear
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 80: 26x26x128 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 81: 26x26x128 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 82: add output layer 79 (26x26x128) to output layer 81 (26x26x128) -> 26x26x128
# [shortcut]
# from=-3
# activation=linear
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 83: 26x26x128 -> 26x26x128
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=mish

# layer 84: concatenate output layer 56 (26x26x128) with output layer 83 (25x26x128) -> 26x26x256
[route]
layers = -1,-28

# layer 85: 26x26x256 -> 26x26x256
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=mish

# Downsample
# layer 86: 26x26x256 -> 13x13x512
[convolutional]
batch_normalize=1
filters=512
size=3
stride=2
pad=1
activation=mish

# layer 87: 13x13x512 -> 13x13x256
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=mish

# layer 88: restart from layer 86 (13x13x512)
[route]
layers = -2

# layer 89: 13x13x512 -> 13x13x256
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=mish

# layer 90: 13x13x256 -> 13x13x256
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=mish

# layer 91: 13x13x256 -> 13x13x256
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=mish

# layer 92: add output layer 89 (13x13x256) to output layer 91 (13x13x256) -> 13x13x256
[shortcut]
from=-3
activation=linear

# layer 93: 13x13x256 -> 13x13x256
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=mish

# layer 94: 13x13x256 -> 13x13x256
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=mish

# layer 95: add output layer 92 (13x13x256) to output layer 94 (13x13x256) -> 13x13x256
[shortcut]
from=-3
activation=linear

# layer 96: 13x13x256 -> 13x13x256
# [convolutional]
# batch_normalize=1
# filters=256
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 97: 13x13x256 -> 13x13x256
# [convolutional]
# batch_normalize=1
# filters=256
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 98: add output layer 95 (13x13x256) to output layer 97 (13x13x256) -> 13x13x256
# [shortcut]
# from=-3
# activation=linear
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 99: 13x13x256 -> 13x13x256
# [convolutional]
# batch_normalize=1
# filters=256
# size=1
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 100: 13x13x256 -> 13x13x256
# [convolutional]
# batch_normalize=1
# filters=256
# size=3
# stride=1
# pad=1
# activation=mish
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 101: add output layer 98 (13x13x256) to output layer 100 (13x13x256) -> 13x13x256
# [shortcut]
# from=-3
# activation=linear
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 102: 13x13x256 -> 13x13x256
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=mish

# layer 103: concatenate output layer 87 (13x13x256) with output layer 102 (13x13x256) -> 13x13x512
[route]
layers = -1,-16

# layer 104: 13x13x512 -> 13x13x512
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=mish

##########################
# TODO: what does this barier mean?

# layer 105: 13x13x512 -> 13x13x256
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# layer 106: 13x13x256 -> 13x13x512
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

# layer 107: 13x13x512 -> 13x13x256
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

### SPP (spatial pyramid pooling) ###
# layer 108: 13x13x256 -> 13x13x256
[maxpool]
stride=1
size=5

# layer 109: restart from output layer 107 (13x13x256)
[route]
layers=-2

[maxpool]
# layer 110: 13x13x256 -> 13x13x256
stride=1
size=9

# layer 111: restart from output layer 107 (13x13x256)
[route]
layers=-4

# layer 112: 13x13x256 -> 13x13x256
[maxpool]
stride=1
size=13

[route]
# layer 113: concatenate 
#   - output layer 107 (13x13x256)
#   - output layer 108 (13x13x256)
#   - output layer 110 (13x13x256)
#   - output lyaer 112 (13x13x256)
#  -> 13x13x1024 
layers=-1,-3,-5,-6
### End SPP ###

# layer 114: 13x13x1024 -> 13x13x256
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# layer 115: 13x13x1024 -> 13x13x512
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

# layer 116: 13x13x512 -> 13x13x256
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# layer 117: 13x13x256 -> 13x13x128
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# layer 118: 13x13x128 -> 26x26x128
[upsample]
stride=2

# layer 119: restart from output layer 85 (26x26x256)
[route]
layers = 85

# layer 120: 26x26x256 -> 26x26x128
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# layer 121: concatenate output layer 118 (26x26x128) with output layer 120 (26x26x128) -> (26x26x256)
[route]
layers = -1, -3

# layer 122: 26x26x256 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=1
# stride=1
# pad=1
# activation=leaky
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 123: 26x26x128 -> 26x26x256
# [convolutional]
# batch_normalize=1
# size=3
# stride=1
# pad=1
# filters=256
# activation=leaky
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 124: 26x26x256 -> 26x26x128
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# layer 125: 26x26x128 -> 26x26x256
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

# layer 126: 26x26x256 -> 26x26x128
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# layer 127: 26x26x128 -> 26x26x64
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

# layer 128: 26x26x64 -> 52x52x64
[upsample]
stride=2

# layer 129: restart from layer 54 (52x52x128)
[route]
layers = 54

# layer 130: 52x52x128 -> 52x52x64
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

# layer 131: concatenate layer 128 (52x52x64) with output layer 130 (52x52x64) -> (52x52x128)
[route]
layers = -1, -3

# layer 132: 52x52x128 -> 52x52x64
# [convolutional]
# batch_normalize=1
# filters=64
# size=1
# stride=1
# pad=1
# activation=leaky
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 133: 52x52x64 -> 52x52x128
# [convolutional]
# batch_normalize=1
# size=3
# stride=1
# pad=1
# filters=128
# activation=leaky
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 134: 52x52x128 -> 52x52x64
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

# layer 135: 52x52x64 -> 52x52x128
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=128
activation=leaky

# layer 136: 52x52x128 -> 52x52x64
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

##########################
# TODO: what does this barrier mean

# layer 137: 52x52x64 -> 52x52x128
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=128
activation=leaky

# layer 138: 52x52x128 -> 52x52x30
[convolutional]
size=1
stride=1
pad=1
filters=30
activation=linear

# layer 139: 52x52x30 -> ?? = RESULT ??
[yolo]
mask = 0,1,2
# use anchors 12, 16, 19, 36, 40, 28
anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401
classes=5
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
scale_x_y = 1.2
iou_thresh=0.213
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=ciou
nms_kind=greedynms
beta_nms=0.6
max_delta=5

# layer 140: restart from output layer 136 (52x52x64)
[route]
layers = -4

# layer 141: 52x52x64 -> 26x26x128
[convolutional]
batch_normalize=1
size=3
stride=2
pad=1
filters=128
activation=leaky

# layer 142: concatenate output layer 126 (26x26x128) with output layer 141 (26x26x128) -> (26x26x256)
[route]
layers = -1, -16

# layer 143: 26x26x256 -> 26x26x128
# [convolutional]
# batch_normalize=1
# filters=128
# size=1
# stride=1
# pad=1
# activation=leaky
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 144: 26x26x128 -> 26x26x256
# [convolutional]
# batch_normalize=1
# size=3
# stride=1
# pad=1
# filters=256
# activation=leaky
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 145: 26x26x256 -> 26x26x128
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# layer 146: 26x26x128 -> 26x26x256
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

# layer 147: 26x26x256 -> 26x26x128
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# layer 148: 26x26x128 -> 26x26x256
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

# layer 149: 26x26x256 -> 26x26x30
[convolutional]
size=1
stride=1
pad=1
filters=30
activation=linear

# layer 150: 26x26x30 -> ?? = RESULT ??
[yolo]
mask = 3,4,5
# use anchors 36, 75, 76, 55, 72, 146
anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401
classes=5
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
scale_x_y = 1.1
iou_thresh=0.213
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=ciou
nms_kind=greedynms
beta_nms=0.6
max_delta=5

# layer 151: restart from output layer 147 (26x26x128)
[route]
layers = -4

# layer 152: 26x26x128 -> 13x13x256
[convolutional]
batch_normalize=1
size=3
stride=2
pad=1
filters=256
activation=leaky

# layer 153: concatenate output layer 116 (13x13x256) with layer 152 (l3x13x256) -> 13x13x512
[route]
layers = -1, -37

# layer 154: 13x13x512 -> 13x13x256
# [convolutional]
# batch_normalize=1
# filters=256
# size=1
# stride=1
# pad=1
# activation=leaky
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 155: 13x13x256 -> 13x13x512
# [convolutional]
# batch_normalize=1
# size=3
# stride=1
# pad=1
# filters=512
# activation=leaky
# replace convolution with shortcut to reduce processing power
[shortcut]
from=-1
activation=linear

# layer 156: 13x13x512 -> 13x13x256
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# layer 157: 13x13x256 -> 13x13x512
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

# layer 158: 13x13x512 -> 13x13x256
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# layer 159: 13x13x256 -> 13x13x512
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

# layer 160: 13x13x512 -> 13x13x30
[convolutional]
size=1
stride=1
pad=1
filters=30
activation=linear

# layer 161: 13x13x30 -> ?? = RESULT ??
[yolo]
mask = 6,7,8
# use anchors 142, 110, 192, 243, 459, 401
anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401
classes=5
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1
scale_x_y = 1.05
iou_thresh=0.213
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=ciou
nms_kind=greedynms
beta_nms=0.6
max_delta=5

